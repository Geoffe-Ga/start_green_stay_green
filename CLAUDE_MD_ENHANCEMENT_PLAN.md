# CLAUDE.md Enhancement Plan

**Date**: 2026-01-13
**Target**: Start Green Stay Green CLAUDE.md
**Goal**: Create best-in-class CLAUDE.md by integrating patterns from WavelengthWatch, specinit, wrist-arcana, and ml-odyssey

---

## Overview

This plan details specific changes to enhance SGSG's CLAUDE.md based on cross-repository analysis. The focus is on **high-impact improvements** that enhance AI agent effectiveness while maintaining SGSG's existing strengths.

---

## Summary of Changes

| Category | Additions | Modifications | Removals |
|----------|-----------|---------------|----------|
| **New Sections** | 4 | - | - |
| **Enhanced Sections** | - | 6 | - |
| **Reorganized Sections** | - | All | 0 |
| **Total Changes** | 10 sections |  | None |

**Total Estimated Effort**: 6-8 hours
**Backward Compatibility**: 100% (no breaking changes)

---

## Phase 4: Implementation Plan

### Part 1: Structural Reorganization (30 min)

**Current State**: SGSG CLAUDE.md has ~20 sections with unclear hierarchy

**Target State**: 10 clear numbered sections with logical flow

**Changes Required**:

1. **Add section numbers** to all top-level sections
2. **Merge related sections** (quality content, testing content)
3. **Create clear hierarchy** with 3 levels maximum (##, ###, ####)

**New Structure** (complete outline):

```markdown
# Claude Code Project Context: Start Green Stay Green

[Table of Contents - AUTO-GENERATED]

## 1. Critical Principles ‚Üê NEW
[5-7 non-negotiable rules]

## 2. Project Overview
[KEEP existing content]

## 3. The Maximum Quality Engineering Mindset
[KEEP existing content]

## 4. Stay Green Workflow
[KEEP existing content, ADD diagrams]

## 5. Architecture
### 5.1 Core Philosophy
### 5.2 Component Structure
### 5.3 Design Patterns ‚Üê NEW
[KEEP existing + ADD generator patterns]

## 6. Quality Standards
### 6.1 Code Quality Requirements
### 6.2 Enforcement Mechanisms
### 6.3 Security Guidelines ‚Üê NEW
[MERGE existing quality sections + ADD security]

## 7. Development Workflow
### 7.1 Branch Strategy
### 7.2 Commit Conventions
### 7.3 Pull Request Process
### 7.4 Code Review Checklist ‚Üê NEW
[KEEP existing workflow content]

## 8. Testing Strategy
### 8.1 Test Organization
### 8.2 Test Structure (AAA Pattern) ‚Üê NEW
### 8.3 Mocking Patterns ‚Üê NEW
### 8.4 Coverage Targets
### 8.5 Mutation Testing
[ENHANCE with examples and patterns]

## 9. Tool Usage & Code Standards
### 9.1 Tool Invocation Patterns
### 9.2 Python Code Style
### 9.3 Generator Patterns ‚Üê NEW
### 9.4 AI Integration Patterns ‚Üê NEW
### 9.5 Template Patterns ‚Üê NEW
### 9.6 Docstring Format
[KEEP + ENHANCE with code examples]

## 10. Common Pitfalls & Troubleshooting
### 10.1 No Shortcuts Policy
### 10.2 Forbidden Patterns
### 10.3 Most Common Mistakes ‚Üê NEW
### 10.4 Debugging Guide
[MERGE existing pitfalls sections]

## Appendix A: AI Subagent Guidelines
[KEEP existing content]

## Appendix B: Skills Catalog ‚Üê NEW
[ADD automation skills]

## Appendix C: Key Files
[KEEP existing]

## Appendix D: External References
[KEEP existing]
```

---

### Part 2: New Section - Critical Principles (1 hour)

**Location**: After Project Overview, before Maximum Quality Mindset

**Content** (complete section):

```markdown
## 1. Critical Principles

These principles are **non-negotiable** and must be followed without exception:

### 1. Use Project Scripts, Not Direct Tools

Always invoke tools through `./scripts/*` instead of directly.

**Why**: Scripts ensure consistent configuration across local development and CI.

| Task | ‚ùå NEVER | ‚úÖ ALWAYS |
|------|----------|-----------|
| Format code | `black .` | `./scripts/format.sh` |
| Run tests | `pytest` | `./scripts/test.sh` |
| Type check | `mypy .` | `./scripts/lint.sh` (includes mypy) |
| Lint code | `ruff check .` | `./scripts/lint.sh` |
| All checks | *(run each tool)* | `./scripts/check-all.sh` |

See [Tool Invocation Patterns](#91-tool-invocation-patterns) for complete list.

---

### 2. DRY Principle - Single Source of Truth

Never duplicate content. Always reference the canonical source.

**Examples**:
- ‚úÖ Stay Green documentation ‚Üí `/reference/workflows/stay-green.md` (single source)
- ‚úÖ Other files ‚Üí Link to stay-green.md
- ‚ùå Copy workflow steps into multiple files

**Why**: Duplicated docs get out of sync, causing confusion and errors.

---

### 3. No Shortcuts - Fix Root Causes

Never bypass quality checks or suppress errors without justification.

**Forbidden Shortcuts**:
- ‚ùå Commenting out failing tests
- ‚ùå Adding `# noqa` without issue reference
- ‚ùå Lowering quality thresholds to pass builds
- ‚ùå Using `git commit --no-verify` to skip pre-commit
- ‚ùå Deleting code to reduce complexity metrics

**Required Approach**:
- ‚úÖ Fix the failing test or mark with `@pytest.mark.skip(reason="Issue #N")`
- ‚úÖ Refactor code to pass linting (or justify with issue: `# noqa  # Issue #N: reason`)
- ‚úÖ Write tests to reach 90% coverage
- ‚úÖ Always run pre-commit checks
- ‚úÖ Refactor complex functions into smaller ones

See [No Shortcuts Allowed](#101-no-shortcuts-policy) for detailed examples.

---

### 4. Stay Green - Never Request Review with Failing Checks

Follow the 4-gate workflow rigorously.

**The Rule**:
- üö´ **NEVER** create PR while CI is red
- üö´ **NEVER** request review with failing checks
- üö´ **NEVER** merge without LGTM

**The Process**:
1. Gate 1: Local checks pass (`./scripts/check-all.sh` ‚Üí exit 0)
2. Gate 2: CI pipeline green (all jobs ‚úÖ)
3. Gate 3: Mutation score ‚â•80%
4. Gate 4: Code review LGTM

See [Stay Green Workflow](#4-stay-green-workflow) for complete documentation.

---

### 5. Quality First - Meet MAXIMUM QUALITY Standards

Quality thresholds are immutable. Meet them, don't lower them.

**Standards**:
- Test Coverage: ‚â•90%
- Docstring Coverage: ‚â•95%
- Mutation Score: ‚â•80%
- Cyclomatic Complexity: ‚â§10 per function
- Pylint Score: ‚â•9.0

**When code doesn't meet standards**:
- ‚ùå Change `fail_under = 70` in pyproject.toml
- ‚úÖ Write more tests, refactor code, improve quality

See [Quality Standards](#6-quality-standards) for enforcement mechanisms.

---

### 6. Operate from Project Root

Use relative paths from project root. Never `cd` into subdirectories.

**Why**: Ensures commands work in any environment (local, CI, scripts).

**Examples**:
- ‚úÖ `./scripts/test.sh tests/unit/ai/test_orchestrator.py`
- ‚ùå `cd tests/unit/ai && pytest test_orchestrator.py`

**CI Note**: CI always runs from project root. Commands that use `cd` will break in CI.

---

### 7. Verify Before Commit

Run `./scripts/check-all.sh` before every commit. Only commit if exit code is 0.

**Pre-Commit Checklist**:
- [ ] `./scripts/check-all.sh` passes (exit 0)
- [ ] All new functions have tests
- [ ] Coverage ‚â•90% maintained
- [ ] No failing tests
- [ ] Conventional commit message ready

See [Pre-Commit Checklist](#10-common-pitfalls--troubleshooting) for complete list.

---

**These principles are the foundation of MAXIMUM QUALITY ENGINEERING. Follow them without exception.**
```

**Rationale**: Provides immediate clarity on non-negotiables, prevents most common mistakes.

---

### Part 3: New Section - Security Guidelines (1 hour)

**Location**: Within "Quality Standards" section (6.3)

**Content**:

```markdown
### 6.3 Security Guidelines

#### API Key Handling

**NEVER** store API keys in:
- ‚ùå Environment variables (`.env` files - committed by accident)
- ‚ùå Configuration files (hardcoded strings)
- ‚ùå Code files (constants)

**ALWAYS** use OS keyring:

```python
import keyring

# Store API key (one-time setup)
keyring.set_password("sgsg", "claude_api_key", api_key)

# Retrieve API key
api_key = keyring.get_password("sgsg", "claude_api_key")
if not api_key:
    msg = "Claude API key not found in OS keyring"
    raise ValueError(msg)

# Use with AIOrchestrator
orchestrator = AIOrchestrator(api_key=api_key)
```

**Generated Repositories**: Include keyring setup in generated setup scripts.

---

#### Path Validation

**NEVER** trust user-provided paths without validation:

```python
# ‚ùå WRONG: Path traversal vulnerability
def create_file(user_path: str, content: str) -> None:
    with open(user_path, 'w') as f:
        f.write(content)  # User could pass "../../etc/passwd"
```

**ALWAYS** validate and resolve paths:

```python
# ‚úÖ CORRECT: Validate path is within allowed directory
from pathlib import Path

def create_file(user_path: str, content: str, base_dir: Path) -> None:
    """Create file with path traversal protection.

    Args:
        user_path: User-provided path (relative to base_dir).
        content: File content to write.
        base_dir: Base directory (all files must be within this).

    Raises:
        ValueError: If path traversal detected.
    """
    target = (base_dir / user_path).resolve()

    # Ensure resolved path is within base directory
    if not target.is_relative_to(base_dir):
        msg = f"Path traversal detected: {user_path}"
        raise ValueError(msg)

    target.parent.mkdir(parents=True, exist_ok=True)
    target.write_text(content)
```

**Critical for**: Generators that create files based on user input.

---

####Subprocess Safety

**NEVER** use `shell=True`:

```python
# ‚ùå WRONG: Shell injection vulnerability
import subprocess
subprocess.run(f"git clone {user_repo}", shell=True)
```

**ALWAYS** use list form without shell:

```python
# ‚úÖ CORRECT: No shell injection possible
import subprocess
subprocess.run(["git", "clone", user_repo], check=True)
```

**Why**: `shell=True` allows shell metacharacters (`; | & $`) to execute arbitrary commands.

---

#### Input Validation

**NEVER** assume user input is valid:

```python
# ‚ùå WRONG: No validation
def generate_project(name: str) -> None:
    subprocess.run(["mkdir", name])  # Name could be malicious
```

**ALWAYS** validate input:

```python
# ‚úÖ CORRECT: Validate project name
import re

def validate_project_name(name: str) -> None:
    """Validate project name is safe.

    Args:
        name: Project name to validate.

    Raises:
        ValueError: If name is invalid.
    """
    # Only allow alphanumeric, hyphens, underscores
    if not re.match(r'^[a-zA-Z0-9_-]+$', name):
        msg = f"Invalid project name: {name} (only letters, numbers, hyphens, underscores allowed)"
        raise ValueError(msg)

    if len(name) > 100:
        msg = f"Project name too long: {len(name)} chars (max 100)"
        raise ValueError(msg)

    if name.startswith(('-', '_')):
        msg = f"Project name cannot start with {name[0]}"
        raise ValueError(msg)

def generate_project(name: str) -> None:
    """Generate new project with validated name."""
    validate_project_name(name)
    subprocess.run(["mkdir", name], check=True)
```

**Apply to**: All user inputs (project names, file paths, template variables).

---

#### Secrets in Generated Code

**Never generate code that includes secrets**:

```python
# ‚ùå WRONG: Generate API key in code
template = """
API_KEY = "{{ api_key }}"  # Hardcoded secret!
"""

# ‚úÖ CORRECT: Generate keyring usage
template = """
import keyring
API_KEY = keyring.get_password("{{ project_name }}", "api_key")
if not API_KEY:
    raise ValueError("API key not found in OS keyring")
"""
```

**Generated `.gitignore`** must include:
```gitignore
.env
*.key
*.pem
secrets.yml
credentials.json
```

---

**These security guidelines are critical for a tool that generates code. Follow them rigorously.**
```

**Rationale**: SGSG generates repos with user input - security is essential.

---

### Part 4: Enhanced Section - Testing Strategy (1.5 hours)

**Location**: Section 8

**Changes**:
1. Add AAA pattern examples
2. Add mocking patterns
3. Add coverage targets table
4. Add property-based testing example

**New Content to ADD**:

```markdown
### 8.2 Test Structure (AAA Pattern)

All tests follow **Arrange-Act-Assert** structure for clarity:

```python
def test_generator_creates_valid_ci_workflow():
    """Test CI generator creates valid GitHub Actions workflow."""
    # Arrange: Set up test data and mocks
    generator = CIGenerator(language="python")
    target_path = tmp_path / "output"
    target_path.mkdir()

    # Act: Execute the functionality being tested
    result = generator.generate(target_path)

    # Assert: Verify expected outcomes
    assert result.success
    workflow_file = target_path / ".github" / "workflows" / "ci.yml"
    assert workflow_file.exists()

    workflow = yaml.safe_load(workflow_file.read_text())
    assert workflow["jobs"]["test"]["runs-on"] == "ubuntu-latest"
    assert "pytest" in workflow["jobs"]["test"]["steps"][-1]["run"]
```

**Benefits**:
- **Arrange**: Clear setup makes test reproducible
- **Act**: Single action makes test focused
- **Assert**: Explicit checks make failures obvious

---

### 8.3 Mocking Patterns

#### Mock AI Orchestrator

```python
@pytest.fixture
def mock_orchestrator(mocker):
    """Mock AI orchestrator for generator tests."""
    mock = mocker.Mock(spec=AIOrchestrator)
    mock.generate.return_value = GenerationResult(
        content="# Generated Content\\n\\nThis is a test.",
        format="markdown",
        token_usage=TokenUsage(input_tokens=100, output_tokens=50),
        model="claude-sonnet-4-5-20250929",
        message_id="msg_test123",
    )
    return mock

def test_generator_uses_ai_orchestrator(mock_orchestrator):
    """Test generator calls AI orchestrator with correct prompt."""
    generator = READMEGenerator(orchestrator=mock_orchestrator)

    result = generator.generate(project_name="test-project", language="python")

    # Verify orchestrator was called
    mock_orchestrator.generate.assert_called_once()
    call_args = mock_orchestrator.generate.call_args

    # Verify prompt includes project details
    assert "test-project" in call_args[0][0]  # prompt argument
    assert call_args[1]["output_format"] == "markdown"
```

#### Mock Template Environment

```python
@pytest.fixture
def mock_template_env(tmp_path):
    """Mock Jinja2 environment with test templates."""
    templates_dir = tmp_path / "templates"
    templates_dir.mkdir()

    # Create test template
    (templates_dir / "config.yml.j2").write_text("""
name: {{ project_name }}
language: {{ language }}
""")

    env = Environment(loader=FileSystemLoader(str(templates_dir)))
    return env

def test_generator_renders_template(mock_template_env):
    """Test generator renders template with correct context."""
    template = mock_template_env.get_template("config.yml.j2")
    result = template.render(project_name="test", language="python")

    assert "name: test" in result
    assert "language: python" in result
```

#### Mock File System Operations

```python
def test_generator_creates_directory_structure(tmp_path, mocker):
    """Test generator creates expected directory structure."""
    # Mock Path.mkdir to track calls
    mkdir_spy = mocker.spy(Path, "mkdir")

    generator = ScaffoldGenerator()
    generator.create_structure(tmp_path)

    # Verify mkdir called for each expected directory
    expected_dirs = ["src", "tests", "docs", "scripts"]
    for dir_name in expected_dirs:
        assert any(
            str(call[0][0]).endswith(dir_name)
            for call in mkdir_spy.call_args_list
        )
```

---

### 8.4 Coverage Targets

| Component Type | Minimum | Target | Rationale |
|----------------|---------|--------|-----------|
| **Generators** | 95% | 98%+ | Core functionality, must be bulletproof |
| **AI Integration** | 90% | 95%+ | Complex logic, many edge cases |
| **Utils** | 90% | 95%+ | Widely reused, bugs multiply |
| **CLI** | 80% | 85%+ | User-facing, some UI code hard to test |
| **Configuration** | 85% | 90%+ | Critical for project setup |
| **Templates** | N/A | N/A | Tested via integration tests |

**Overall Project**: 90% minimum, 97%+ target (current: 97.22%)

**Enforcement**: `pytest --cov-fail-under=90` in `scripts/test.sh`

---

### 8.5 Property-Based Testing

Use Hypothesis for generators to test invariants:

```python
from hypothesis import given, strategies as st

@given(
    language=st.sampled_from(["python", "typescript", "go", "rust"]),
    project_name=st.text(
        min_size=1,
        max_size=50,
        alphabet=st.characters(
            whitelist_categories=("Lu", "Ll", "Nd"),
            blacklist_characters="/\\:*?\"<>|",
        ),
    ),
)
def test_generator_handles_all_valid_inputs(language, project_name):
    """Test generator handles all combinations of valid inputs."""
    generator = ProjectGenerator(language=language)

    # Should either succeed or fail with clear error
    try:
        result = generator.validate_name(project_name)
        assert result.is_valid
    except ValueError as e:
        # If validation fails, error message must be clear
        assert project_name in str(e) or language in str(e)

@given(
    config=st.fixed_dictionaries({
        "name": st.text(min_size=1, max_size=100),
        "language": st.sampled_from(["python", "typescript", "go"]),
        "include_ci": st.booleans(),
        "include_tests": st.booleans(),
    })
)
def test_config_generator_is_idempotent(config, tmp_path):
    """Test generator produces same output for same input."""
    generator = ConfigGenerator()

    # Generate twice with same input
    result1 = generator.generate(tmp_path / "out1", **config)
    result2 = generator.generate(tmp_path / "out2", **config)

    # Outputs should be identical
    assert result1.files_created == result2.files_created
    for file_name in result1.files_created:
        content1 = (tmp_path / "out1" / file_name).read_text()
        content2 = (tmp_path / "out2" / file_name).read_text()
        assert content1 == content2
```

**When to Use**:
- Testing invariants (idempotency, commutativity)
- Validating input handling across wide range
- Finding edge cases you didn't think of

---
```

**Rationale**: Makes testing requirements concrete and actionable.

---

### Part 5: Code Examples Throughout (2 hours)

**Locations**: Sections 9.3, 9.4, 9.5 (NEW subsections)

**Content**:

**9.3 Generator Patterns**:

```markdown
### 9.3 Generator Patterns

#### Pattern 1: Template-Based Generation

‚ùå **WRONG**: String concatenation
```python
def generate_readme(name):
    return "# " + name + "\\n\\n## Overview\\n"
```

‚úÖ **CORRECT**: Jinja2 templates
```python
class READMEGenerator:
    def __init__(self, templates_dir: Path):
        self.env = Environment(loader=FileSystemLoader(str(templates_dir)))

    def generate(self, name: str, language: str) -> str:
        """Generate README using template."""
        template = self.env.get_template("README.md.j2")
        return template.render(project_name=name, language=language)
```

#### Pattern 2: AI-Assisted Generation

‚úÖ **CORRECT**: Use AIOrchestrator
```python
def generate_with_ai(
    self,
    prompt_template: str,
    context: dict[str, str],
) -> GenerationResult:
    """Generate content using AI with template."""
    # Render prompt with context
    prompt = self.prompt_manager.render(prompt_template, context)

    # Generate with retry logic
    result = self.orchestrator.generate(
        prompt=prompt,
        output_format="markdown",
    )

    return result
```

#### Pattern 3: Validation

‚úÖ **CORRECT**: Validate before and after
```python
def generate(self, target: Path, config: dict) -> GenerationResult:
    """Generate with validation."""
    # Validate inputs
    self._validate_config(config)
    self._validate_target(target)

    # Generate files
    files_created = self._create_files(target, config)

    # Validate outputs
    self._validate_generated_files(files_created)

    return GenerationResult(success=True, files=files_created)
```
```

**9.4 AI Integration Patterns**:

```markdown
### 9.4 AI Integration Patterns

#### Pattern 1: Error Handling with Retry

‚úÖ **CORRECT**: Handle API errors gracefully
```python
def generate_with_retries(
    self,
    prompt: str,
    max_retries: int = 3,
) -> GenerationResult:
    """Generate with exponential backoff retry."""
    for attempt in range(max_retries + 1):
        try:
            return self.orchestrator.generate(prompt, "markdown")
        except GenerationError as e:
            if attempt == max_retries:
                raise
            delay = 2 ** attempt
            logger.warning(f"API error, retrying in {delay}s: {e}")
            time.sleep(delay)
```

#### Pattern 2: Prompt Management

‚úÖ **CORRECT**: Use PromptManager
```python
# In prompt_manager.py
class PromptManager:
    def render_and_generate(
        self,
        template_name: str,
        context: dict[str, str],
    ) -> GenerationResult:
        """Render prompt template and generate content."""
        prompt = self.render(template_name, context)
        return self.orchestrator.generate(prompt, context.get("format", "markdown"))
```

#### Pattern 3: Response Validation

‚úÖ **CORRECT**: Validate AI responses
```python
def generate_yaml_config(self, prompt: str) -> dict:
    """Generate YAML config, validate structure."""
    result = self.orchestrator.generate(prompt, "yaml")

    # Parse and validate
    try:
        config = yaml.safe_load(result.content)
    except yaml.YAMLError as e:
        raise ValueError(f"AI generated invalid YAML: {e}") from e

    # Validate required fields
    required = {"name", "version", "dependencies"}
    if not required.issubset(config.keys()):
        raise ValueError(f"Missing required fields: {required - config.keys()}")

    return config
```
```

**9.5 Template Patterns**:

```markdown
### 9.5 Template Patterns (Jinja2)

#### Variable Interpolation

```jinja2
# Template: README.md.j2
# {{ project_name }}

> {{ description }}

## Installation

```bash
pip install {{ project_name }}
```

## Usage

```{{ language }}
import {{ project_name.replace('-', '_') }}
```
```

#### Conditionals

```jinja2
{%- if include_ci %}
## CI/CD

This project uses GitHub Actions for continuous integration.

See `.github/workflows/ci.yml` for details.
{% endif -%}

{%- if language == "python" %}
## Development Setup

```bash
python -m venv .venv
source .venv/bin/activate
pip install -e ".[dev]"
```
{%- elif language == "typescript" %}
## Development Setup

```bash
npm install
npm run dev
```
{%- endif -%}
```

#### Loops

```jinja2
## Features

{% for feature in features %}
- {{ feature.name }}: {{ feature.description }}
{% endfor %}

## Dependencies

{% for dep, version in dependencies.items() %}
- `{{ dep }}`: {{ version }}
{% endfor %}
```

#### Filters

```jinja2
# Convert to snake_case
module_name = "{{ project_name | replace('-', '_') }}"

# Capitalize first letter
ClassName = "{{ project_name | replace('-', ' ') | title | replace(' ', '') }}"

# Default values
description = "{{ description | default('No description provided') }}"

# List operations
authors = {{ authors | join(', ') }}
```

#### Template Inheritance

```jinja2
{# base.md.j2 #}
# {{ project_name }}

{% block description %}
{{ description }}
{% endblock %}

{% block content %}
{% endblock %}

---
Generated with Start Green Stay Green
```

```jinja2
{# README.md.j2 #}
{% extends "base.md.j2" %}

{% block content %}
## Installation
...

## Usage
...
{% endblock %}
```
```

**Rationale**: Concrete examples make abstract concepts actionable.

---

### Part 6: Most Common Mistakes Section (1 hour)

**Location**: Section 10.3 (NEW)

**Content**:

```markdown
### 10.3 Most Common Mistakes

Based on PR review analysis, these are the top mistakes (with frequency):

#### 1. Skipping Local Quality Checks (35%)

**The Mistake**:
```bash
# Committing without running checks
git add .
git commit -m "feat: add feature"
git push
# ‚Üí CI fails with linting errors 5 minutes later
```

**The Fix**:
```bash
# ALWAYS run checks before committing
./scripts/check-all.sh
# Only commit if exit code is 0
git add .
git commit -m "feat(generators): add CI generator (#46)"
```

**Why It Happens**: Impatience, assuming "it's a small change"
**Prevention**: Add pre-commit hook, build muscle memory

---

#### 2. Lowering Quality Thresholds (25%)

**The Mistake**:
```toml
# In pyproject.toml
[tool.coverage.report]
fail_under = 70  # ‚Üê Changed from 90 to make build pass
```

**The Fix**:
```python
# Write tests to reach 90% coverage
def test_edge_case_not_previously_covered():
    """Test edge case that was missing coverage."""
    result = handle_edge_case(unusual_input)
    assert result.is_valid()
```

**Why It Happens**: Deadline pressure, thinking "I'll fix it later"
**Prevention**: Treat thresholds as immutable

---

#### 3. Using Direct Tool Invocation (20%)

**The Mistake**:
```bash
# Running tools directly
ruff check .
pytest tests/
mypy start_green_stay_green/
```

**The Fix**:
```bash
# Use project scripts
./scripts/check-all.sh  # Runs all tools with correct config
```

**Why It Happens**: Muscle memory from other projects
**Prevention**: Read Tool Invocation Patterns section

---

#### 4. Commenting Out Failing Tests (15%)

**The Mistake**:
```python
# def test_important_feature():
#     """This test is failing, commenting out for now."""
#     assert process_data(input).is_valid()
```

**The Fix**:
```python
@pytest.mark.skip(reason="Issue #123: Waiting for API endpoint")
def test_important_feature():
    """Test important feature works correctly."""
    assert process_data(input).is_valid()
```

**Why It Happens**: Test fails, don't know how to fix immediately
**Prevention**: Use `@pytest.mark.skip` with issue reference

---

#### 5. Adding # noqa Without Justification (5%)

**The Mistake**:
```python
very_long_variable_name = some_function(arg1, arg2, arg3)  # noqa: E501
```

**The Fix**:
```python
# Option 1: Refactor
very_long_name = some_function(
    arg1, arg2, arg3
)

# Option 2: If unavoidable, justify
api_url = "https://..."  # noqa: E501  # Issue #42: API URL from spec
```

**Why It Happens**: Easier to suppress than fix
**Prevention**: Require issue number for all noqa

---

#### Summary

| Mistake | Frequency | Avg Fix Time | Impact |
|---------|-----------|--------------|--------|
| Skip local checks | 35% | 5 min | High (wastes CI time) |
| Lower thresholds | 25% | 30 min | High (technical debt) |
| Direct tools | 20% | 2 min | Low (inconsistency) |
| Comment tests | 15% | 15 min | Medium (false coverage) |
| Unjustified noqa | 5% | 5 min | Low (code smell) |

**Total time saved by avoiding these**: ~1 hour per PR on average
```

**Rationale**: Helps developers learn from common mistakes faster.

---

## Backward Compatibility

**All changes are additive**. No existing content is removed, only:
- Reorganized for clarity
- Enhanced with examples
- Supplemented with new sections

**AI Agents**: Existing agent references continue to work. New sections provide additional guidance.

**Generated Repos**: Template updates are optional enhancements.

---

## Implementation Checklist

### Phase 4A: Structural Changes (Day 1 - 2 hours)

- [ ] Add section numbers to all top-level headings
- [ ] Reorganize into 10 main sections + appendices
- [ ] Update internal cross-references
- [ ] Generate table of contents

### Phase 4B: New Content (Day 1-2 - 4 hours)

- [ ] Add Critical Principles section (1 hour)
- [ ] Add Security Guidelines section (1 hour)
- [ ] Enhance Testing Strategy section (1.5 hours)
- [ ] Add Most Common Mistakes section (0.5 hours)

### Phase 4C: Code Examples (Day 2 - 2 hours)

- [ ] Add Generator Patterns examples (0.75 hours)
- [ ] Add AI Integration Patterns examples (0.75 hours)
- [ ] Add Template Patterns examples (0.5 hours)

### Phase 4D: Final Review (Day 2 - 1 hour)

- [ ] Review all new content for accuracy
- [ ] Verify all cross-references work
- [ ] Test code examples compile/run
- [ ] Update reference/claude/CLAUDE.md template with same changes

---

## Success Metrics

The enhanced CLAUDE.md is successful when:

1. **Critical Principles** section prevents top 5 mistakes
2. **Code Examples** reduce "how do I..." questions by 50%
3. **Security Guidelines** prevent vulnerabilities in generated code
4. **Testing Strategy** examples enable faster test writing
5. **Navigation** (numbered sections) reduces time to find info by 30%

---

## Timeline

| Phase | Duration | Deliverable |
|-------|----------|-------------|
| Phase 4A | 2 hours | Reorganized structure |
| Phase 4B | 4 hours | New content sections |
| Phase 4C | 2 hours | Code examples |
| Phase 4D | 1 hour | Review & validation |
| **Total** | **8-9 hours** | **Amalgamated CLAUDE.md** |

---

## Next Steps

1. **Get User Approval**: Review this plan before implementation
2. **Implement Phase 4**: Execute changes in order (A ‚Üí B ‚Üí C ‚Üí D)
3. **Update Template**: Apply same changes to `/reference/claude/CLAUDE.md`
4. **Validate**: Test with AI agents to verify effectiveness
5. **Document**: Create changelog explaining all changes

---

**This plan integrates the best practices from WavelengthWatch, specinit, wrist-arcana, and ml-odyssey while preserving SGSG's existing strengths. The result will be a best-in-class CLAUDE.md that serves both SGSG development and all generated repositories.**
